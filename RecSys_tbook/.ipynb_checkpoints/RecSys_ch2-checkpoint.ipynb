{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System\n",
    "---\n",
    "\n",
    "## Ch2 Neighborhood-based CF\n",
    "### 2.1 Introduction\n",
    "1. keywords:\n",
    "    * basic idea: similar user have simlar rates; similar item share similar rates\n",
    "    * two type algos: user-based CF, item-based CF\n",
    "    * two type form: rate matrix completion; Top K\n",
    "    * key properties of rate matrices\n",
    "    * optimization view\n",
    "    * graph view\n",
    "1. que:\n",
    "    1. difference between user-based CF and item-based\n",
    "    1. what's the basic idea of memory-based CF(2 sentences)\n",
    "    \n",
    "### 2.2 Key Properties of Ratings matrices\n",
    "1. que:\n",
    "    1. how to split rating matrix into train/test sets under classification framework\n",
    "    > <img src=\"img/cf_compare_classification.png\" width=\"50%\" height=\"50%\">\n",
    "    1. how many ways to define rating\n",
    "1. keywords:\n",
    "    1. rating types: \n",
    "        * continuous\n",
    "        * interval-based\n",
    "        * ordinal\n",
    "        * binary\n",
    "        * unary\n",
    "    1. 2 key properties\n",
    "        1. sparsity\n",
    "        1. **long-tail**\n",
    "            1. <span style=\"color:#E6500A\">value lies in low frequent items, not in high-freq</span>\n",
    "            1. <span style=\"color:#E6500A\">most recommender tends to recommend high-freq, which reduces diversity</span>\n",
    "            1. <span style=\"color:#E6500A\">mis-representative: high-frequent items cannot represent low-freq items</span>\n",
    "        > <img src=\"img/long-tail.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "### 2.3 Neighborhood-Based Methods\n",
    "\n",
    "#### 2.3.1 User-Based\n",
    "1. basic idea: identify similar users to target users --> similarity function\n",
    "1. procedure:\n",
    "    1. **compute mean**: $$\\mu_{u}=\\frac{\\sum_{k \\in I_{u}} r_{u k}}{\\left|I_{u}\\right|} \\quad \\forall u \\in\\{1 \\ldots m\\}$$\n",
    "    1. **normalize**: $$s_{u j}=r_{u j}-\\mu_{u} \\quad \\forall u \\in\\{1 \\ldots m\\}$$\n",
    "    1. **compute similarity func**(Pearson): $$\\operatorname{sim}(u, v)=\\operatorname{Pearson}(u, v)=\\frac{\\sum_{k \\in I_{u} \\cap I_{v}}\\left(r_{u k}-\\mu_{u}\\right) \\cdot\\left(r_{v k}-\\mu_{v}\\right)}{\\sqrt{\\sum_{k \\in I_{u} \\cap I_{v}}\\left(r_{u k}-\\mu_{u}\\right)^{2}} \\cdot \\sqrt{\\sum_{k \\in I_{u} \\cap I_{v}}\\left(r_{v k}-\\mu_{v}\\right)^{2}}}$$\n",
    "    1. **compute prediction score(mean-centric raw rating)**: $$\\hat{r}_{u j}=\\mu_{u}+\\frac{\\sum_{v \\in P_{u}(j)} \\operatorname{Sim}(u, v) \\cdot s_{v j}}{\\sum_{v \\in P_{u}(j)}|\\operatorname{Sim}(u, v)|}=\\mu_{u}+\\frac{\\sum_{v \\in P_{u}(j)} \\operatorname{Sim}(u, v) \\cdot\\left(r_{v j}-\\mu_{v}\\right)}{\\sum_{v \\in P_{u}(j)}|\\operatorname{Sim}(u, v)|}$$\n",
    "1. similarity function variant:\n",
    "    1. significance weight: numbers of $\\left|I_{u} \\cap I_{v}\\right|$ often affect similarity coefficient, use a threshold $\\beta$ to control significance: $$\\operatorname{Sim}(u, v)=\\operatorname{Sim}(u, v) \\cdot \\frac{\\min \\left\\{\\left|I_{u} \\cap I_{v}\\right|, \\beta\\right\\}}{\\beta}$$\n",
    "1. prediction func variants:\n",
    "    1. Z-score: normalization\n",
    "    > $\\sigma_{u}=\\sqrt{\\frac{\\sum_{j \\in I_{u}}\\left(r_{u j}-\\mu_{u}\\right)^{2}}{\\left|I_{u}\\right|-1}} \\quad \\forall u \\in\\{1 \\ldots m\\}$<br>\n",
    "    > $z_{u j}=\\frac{r_{u j}-\\mu_{u}}{\\sigma_{u}}=\\frac{s_{u j}}{\\sigma_{u}}$<br>\n",
    "    > $\\hat{r}_{u j}=\\mu_{u}+\\sigma_{u} \\frac{\\sum_{v \\in P_{u}(j)} \\operatorname{Sim}(u, v) \\cdot z_{v j}}{\\sum_{v \\in P_{u}(j)}|\\operatorname{Sim}(u, v)|}$\n",
    "    1. exponential amplifying similarity\n",
    "    > $\\operatorname{Sim}(u, v)=\\operatorname{Pearson}(u, v)^{\\alpha}$\n",
    "1. peer group filtering variations:\n",
    "    > Filter out weak or negative correlation in top@K\n",
    "1. long-tail effect\n",
    "    * phenomenon: the more some item appears, the less value it holds.\n",
    "    * solution: inverse user frequence(iuf, borrowed from information retrieval field)\n",
    "    > $w_{j}=\\log \\left(\\frac{m}{m_{j}}\\right) \\quad \\forall j \\in\\{1 \\ldots n\\}$<br>\n",
    "    > $\\operatorname{Pearson}(u, v)=\\frac{\\sum_{k \\in I_{u} \\cap I_{v}} w_{k} \\cdot\\left(r_{u k}-\\mu_{u}\\right) \\cdot\\left(r_{v k}-\\mu_{v}\\right)}{\\sqrt{\\sum_{k \\in I_{u} \\cap I_{v}} w_{k} \\cdot\\left(r_{u k}-\\mu_{u}\\right)^{2}} \\cdot \\sqrt{\\sum_{k \\in I_{u} \\cap I_{v}} w_{k} \\cdot\\left(r_{v k}-\\mu_{v}\\right)^{2}}}$\n",
    "\n",
    "1. <span style=\"color:red\">que</span>:\n",
    "    1. cannot understand:\n",
    "    > since the number of observed ratings in the top-k peer group of a target user may vary significantly with the item at hand, the closest k users are found for the target user separately for each predicted item, such that each of these k users have specified ratings for that item.\n",
    "    1. why mean-centric is normalized around the original rate range? Or, is it normalized in rate range?\n",
    "    > So here goes the **Z-score**\n",
    "\n",
    "#### 2.3.2 Item-Based\n",
    "1. keywords:\n",
    "    1. column way of user-based\n",
    "\n",
    "#### 2.3.3 Implementation && Complexity Analysis\n",
    "1. keywords\n",
    "    * offline/online mode\n",
    "\n",
    "#### 2.3.4 Comparation\n",
    "1. keywords:\n",
    "    1. item-based:\n",
    "        1. robust\n",
    "            1. #user_num >> #item_num\n",
    "            1. user add/reduce frequently\n",
    "        1. high accuracy\n",
    "        1. good explanation\n",
    "    1. use-based:\n",
    "        1. high diversity\n",
    "\n",
    "#### 2.3.5 Weak & Strong\n",
    "1. keywords:\n",
    "    1. ad:\n",
    "        1. intuitive/simply\n",
    "        1. explanable\n",
    "        1. easy to implement\n",
    "    1. disad:\n",
    "        1. high resource consumption\n",
    "        1. sparsity && cold start\n",
    "\n",
    "#### 2.3.6 Unification View\n",
    "1. keys:\n",
    "    1. assumption: mean-centric\n",
    "    1. unified frame:\n",
    "        1. compute similarity\n",
    "        1. compute similarity-weighted average of rates\n",
    "\n",
    "### 2.4 clustering\n",
    "1. que:\n",
    "    1. clustering means reduce intra-cluster similarity, and increase in-cluster similarity\n",
    "    > actually, prediction only uses inner-cluster.\n",
    "1. summary:\n",
    "    1. Why:\n",
    "        1. original offline mode: inefficience\n",
    "        1. clustering is cheaper than original offline computation\n",
    "    1. how: clustering(K-means, etc) --> inner-clustering neighborhood-based CF\n",
    "\n",
    "### 2.5 dimension reduction   \n",
    "1. keys:\n",
    "    1. why: use major factors to represent rate matrix, and SVD gives this opportunity.\n",
    "    1. two ways:\n",
    "        1. reduction by <span style=\"color:#0AE685\">either</span> row(user)-wise \n",
    "           <span style=\"color:#0AE685\">or</span> column(item)-wise\n",
    "        1. reduction by row-wise and column-wise <span style=\"color:#0AE685\">simultaneously</span>\n",
    "    1. tech: SVD, PCA\n",
    "    1. improvements: bias reduction\n",
    "        1. mean filling: bad, high bias induced\n",
    "        1. maximum likelihood estimation: more reasonable in high covariance sense\n",
    "            1. compute MLE covariance matrix\n",
    "            1. compute $\\mathbf{P}_{d}$ of SVD using covariance matrix\n",
    "            1. further reduce bias by available-rate weighted average $\\mathbf{P}_{d}$: \n",
    "                $$a_{u i}=\\frac{\\sum_{j \\in I_{u}} r_{u j} e_{j i}}{\\left|I_{u}\\right|}$$\n",
    "            1. finally, use $A=\\left[a_{u i}\\right]_{m \\times d}$ as latent rate matrix to compute similarity\n",
    "               and prediction\n",
    "        1. directly matrix factorization under optimization frame, only using observed rates\n",
    "1. que:\n",
    "    1. **[SOLVED]** In order to further reduce the bias in representation, the incomplete matrix $\\mathbf{R}$ \n",
    "       can be directly projected on the reduced matrix $\\mathbf{P}_{d}$, rather than projecting \n",
    "       the filled matrix $\\mathbf{R}_{f}$\n",
    "       > but you don't have $\\mathbf{P}_{d}$ until you fill the incomplete matrix $\\mathbf{R}$ <br>\n",
    "       > <span style=\"color:red\">read clearly and carefully, u just skip important lines of words</span>\n",
    "    1. **[SOLVED]** what's the physical meaning of $R_{f} P_{d}$?\n",
    "    > math is not about symbol manipulation under some rules, it's about accurately describe and predict\n",
    "    everthing around you. So stop thinking in computational way, instead, in conceptual/physical way.\n",
    "\n",
    "### 2.6 regression view\n",
    "1. que:\n",
    "    1. rate distribution range changed after all of this manipulation?\n",
    "    1. predict each rating of target user u with the user-based model of Equation 2.22. \n",
    "       Then, instead of comparing it with the observed value of the same item, we compare\n",
    "       it with the observed ratings of other items of that user.\n",
    "    1. how about user-item joint similarity under semantic meaning?\n",
    "    1. **[SOLVED]** joint interpolation in both direction: user-->item, item-->user\n",
    "1. keys:\n",
    "    1. single-based\n",
    "        1. user-based regression\n",
    "        1. item-based regression\n",
    "    1. combination-based\n",
    "    1. joint interpolation regression\n",
    "    1. sparse linear models(SLIM)\n",
    "        * 3 new assumption: non mean-centric; non-negative preference; zaro fitting\n",
    "        * prediction function: any item and any user on constraint of self exclusion \n",
    "            * scalar version: \n",
    "            $$\\hat{r}_{u t}=\\sum_{j=1}^{n} w_{j t}^{item} \\cdot r_{u j} \\quad \\forall u \\in\\{1 \\ldots m\\}, \\forall t \\in\\{1 \\ldots n\\}$$\n",
    "            * matrix version: \n",
    "            $$\\begin{array}{l}{\\hat{R}=R W^{item}} \\\\ {\\text { Diagonal }\\left(W^{item}\\right)=0}\\end{array}$$\n",
    "        * optimization func:$$\n",
    "\\begin{aligned} \\text { Minimize } J_{t}^{s} &=\\sum_{u=1}^{m}\\left(r_{u t}-\\hat{r}_{u t}\\right)^{2}+\\lambda \\cdot \\sum_{j=1}^{n}\\left(w_{j t}^{i t e m}\\right)^{2}+\\lambda_{1} \\cdot \\sum_{j=1}^{n}\\left|w_{j t}^{i t e m}\\right| \\\\ &=\\sum_{u=1}^{m}\\left(r_{u t}-\\sum_{j=1}^{n} w_{j t}^{i t e m} \\cdot r_{u j}\\right)^{2}+\\lambda \\cdot \\sum_{j=1}^{n}\\left(w_{j t}^{i t e m}\\right)^{2}+\\lambda_{1} \\cdot \\sum_{j=1}^{n}\\left|w_{j t}^{i t e m}\\right| \\\\ & w_{j t}^{i t e m} \\geq 0 \\quad \\forall j \\in\\{1 \\ldots n\\} \\\\ & w_{t t}^{i t e m}=0 \\end{aligned}\n",
    "$$\n",
    "        * difference compared with previous neighborhood-based method:\n",
    "            1. all efficient\n",
    "            1. implicit feedback data sets: positive preference\n",
    "            1. no range adjustion\n",
    "\n",
    "1. summary:\n",
    "> This section reviews neighorhood-based method under regression framework. First gives the single-based method of each one. Then, single-based combination is got. Next, unify each single view under joint interpolation; Finally, a loose-control version of item-based regression called SLIM is presented.\n",
    "\n",
    "### 2.7 Graph Models for Neighborhood-Based Methods\n",
    "> This may answer the 3rd question in 2.6: semantic similarity measure\n",
    "\n",
    "1. general methods to generate graph:\n",
    "    * random-walk\n",
    "    * shortest-path\n",
    "\n",
    "#### 2.7.1 User-Item Graphs\n",
    "> <img src=\"img/user_item.png\" width=\"50%\" height=\"50%\"> \n",
    "\n",
    "1. keys:\n",
    "    1. undirected && bipartite\n",
    "    1. two ways to define neighbors: the other is same as previous Neighborhood-based method\n",
    "        1. random walk\n",
    "        1. Katz Measure:$$Katz(i, j)=\\sum_{t=1}^{\\infty} \\beta^{t} \\cdot n_{i j}^{(t)}$$\n",
    "       \n",
    "#### 2.7.2 User-User Graphs\n",
    "> <img src=\"img/user_user.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "1. que:\n",
    "    1. why linear transformation\n",
    "1. keys:\n",
    "    1. hort && prediction\n",
    "    1. shortest path <-- broad-first algorithm\n",
    "    1. length threshold\n",
    "#### 2.7.3 Item-Item Graphs\n",
    "\n",
    "### 2.8 Summary\n",
    "1. keywords:\n",
    "    1. data sparsity\n",
    "        1. dim reduction\n",
    "        1. graph method: user-user graph, user-item graph, item-item graph\n",
    "    1. generalization of classification/regression: so can be optimized\n",
    "1. que:\n",
    "    1. main challenges of memory-based CF and two of its solution?\n",
    "    \n",
    "### 2.9 Bibliographic Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.54545455 4.36363636]\n",
      " [4.36363636 9.81818182]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "m1 = np.array([1,7,3,5,3,5,3,5,3,5,3,5])\n",
    "m2 = np.array([1,7,1,7,1,7,1,7,1,7,1,7])\n",
    "miss = 4\n",
    "m3 = np.array([1,7,1,7,miss,miss,miss,miss,miss,miss,miss,miss])\n",
    "print(np.cov(m1,m2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
